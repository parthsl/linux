Bottom: 1f28a6ad315bc971074f895c2adedbcf96f77b94
Top:    5eba6c8876cbd7e608bb952d499e3df5213f6965
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-30 14:19:43 +0530

sched/fair: Paraphrase available_idle_cpu to classify cpu_idle_type

The available_idle_cpu() differentiates between the CPUs which are idle or
which has VCPU preempted. With the addition of sched_idle_cpu() checks,
there are several code-paths where this both checks are used in conjunction
with each other. So paraphrase the available_idle_cpu() to allow
classification of different CPU Idle types.

cpu_idle_type is arranged in reverse order of their priority to be used for
task enqueueing, i.e.
1. CPU is busy
2. CPU is idle but VCPU may have been preempted
3. CPU is available for queueing and VCPU is non-preempted
4. CPU is running SCHED_IDLE class task(s)

Signed-off-by: Parth Shah <parth@linux.ibm.com>


---

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 8a72d6e929b3..a691023f5702 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5453,22 +5453,28 @@ static unsigned long capacity_of(int cpu)
 	return cpu_rq(cpu)->cpu_capacity;
 }
 
+enum idle_cpu_type {
+	cpu_busy = 0,
+	cpu_preempted_idle,
+	cpu_non_preempted_idle,
+	cpu_sched_idle
+};
+
 /*
- * available_idle_cpu - is a given CPU idle for enqueuing work.
+ * is_idle_cpu - is a given CPU idle for enqueuing work.
  * @cpu: the CPU in question.
- *
- * Return: 1 if the CPU is currently idle. 0 otherwise.
  */
-int available_idle_cpu(int cpu)
+static int is_idle_cpu(int cpu)
 {
 	if (!idle_cpu(cpu))
-		return 0;
+		return sched_idle_cpu(cpu) ? cpu_sched_idle : cpu_busy;
 
 	if (vcpu_is_preempted(cpu))
-		return 0;
+		return cpu_preempted_idle;
 
-	return 1;
+	return cpu_non_preempted_idle;
 }
+#define available_idle_cpu(cpu) ((is_idle_cpu(cpu)) == cpu_non_preempted_idle)
 
 static void record_wakee(struct task_struct *p)
 {
@@ -5773,7 +5779,7 @@ void __update_idle_core(struct rq *rq)
 		if (cpu == core)
 			continue;
 
-		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 			goto unlock;
 	}
 
@@ -5805,7 +5811,7 @@ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int
 
 		for_each_cpu(cpu, cpu_smt_mask(core)) {
 			__cpumask_clear_cpu(cpu, cpus);
-			if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+			if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 				idle = false;
 		}
 
@@ -5834,7 +5840,7 @@ static int select_idle_smt(struct task_struct *p, int target)
 	for_each_cpu(cpu, cpu_smt_mask(target)) {
 		if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 			continue;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			return cpu;
 	}
 
@@ -5899,7 +5905,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 	for_each_cpu_wrap(cpu, cpus, target) {
 		if (!--nr)
 			return -1;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			break;
 	}
 
@@ -5919,14 +5925,14 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	struct sched_domain *sd;
 	int i, recent_used_cpu;
 
-	if (available_idle_cpu(target) || sched_idle_cpu(target))
+	if (is_idle_cpu(target) >= cpu_non_preempted_idle)
 		return target;
 
 	/*
 	 * If the previous CPU is cache affine and idle, don't be stupid:
 	 */
 	if (prev != target && cpus_share_cache(prev, target) &&
-	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
+	    (is_idle_cpu(prev) >= cpu_non_preempted_idle))
 		return prev;
 
 	/* Check a recently used CPU as a potential idle candidate: */
@@ -5934,7 +5940,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	if (recent_used_cpu != prev &&
 	    recent_used_cpu != target &&
 	    cpus_share_cache(recent_used_cpu, target) &&
-	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
+	    is_idle_cpu(recent_used_cpu) >= cpu_non_preempted_idle &&
 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr)) {
 		/*
 		 * Replace recent_used_cpu with prev as it is a potential
