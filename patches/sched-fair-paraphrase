Bottom: 8e92928fe5613c705f3b9facd7e6b85f724145e9
Top:    b2578af105ba6c38c45331d6fc64dcee25c0fdea
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-30 14:19:43 +0530

sched/fair: Paraphrase available_idle_cpu to classify cpu_idle_type

The available_idle_cpu() differentiates between the CPUs which are idle or
which has VCPU preempted. With the addition of sched_idle_cpu() checks,
there are several code-paths where this both checks are used in conjunction
with each other. So paraphrase the available_idle_cpu() to allow
classification of different CPU Idle types.

cpu_idle_type is arranged in reverse order of their priority to be used for
task enqueueing, i.e.
1. CPU is busy
2. CPU is idle but VCPU may have been preempted
3. CPU is available for queueing and VCPU is non-preempted
4. CPU is running SCHED_IDLE class task(s)

Also, move the available_idle_cpu() definition to the sched/fair.c where
all its callers exists.

Signed-off-by: Parth Shah <parth@linux.ibm.com>


---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fc1dfc007604..30704f2037db 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4642,23 +4642,6 @@ int idle_cpu(int cpu)
 	return 1;
 }
 
-/**
- * available_idle_cpu - is a given CPU idle for enqueuing work.
- * @cpu: the CPU in question.
- *
- * Return: 1 if the CPU is currently idle. 0 otherwise.
- */
-int available_idle_cpu(int cpu)
-{
-	if (!idle_cpu(cpu))
-		return 0;
-
-	if (vcpu_is_preempted(cpu))
-		return 0;
-
-	return 1;
-}
-
 /**
  * idle_task - return the idle task for a given CPU.
  * @cpu: the processor in question.
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6690845f4d79..c880a8225b56 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5453,6 +5453,32 @@ static unsigned long capacity_of(int cpu)
 	return cpu_rq(cpu)->cpu_capacity;
 }
 
+enum cpu_idle_type {
+	cpu_busy = 0,
+	cpu_preempted_idle
+	cpu_non_preempted_idle,
+	cpu_sched_idle,
+}
+
+/*
+ * is_idle_cpu - is a given CPU idle for enqueuing work.
+ * @cpu: the CPU in question.
+ */
+static int is_idle_cpu(int cpu)
+{
+	if (sched_idle_cpu(cpu))
+		return cpu_sched_idle;
+
+	if (!idle_cpu(cpu))
+		return cpu_busy;
+
+	if (vcpu_is_preempted(cpu))
+		return cpu_non_preempted_idle;
+
+	return cpu_preempted_idle;
+}
+#define available_idle_cpu(cpu) ((is_idle_cpu(cpu)) == cpu_non_preempted_idle)
+
 static void record_wakee(struct task_struct *p)
 {
 	/*
@@ -5756,7 +5782,7 @@ void __update_idle_core(struct rq *rq)
 		if (cpu == core)
 			continue;
 
-		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 			goto unlock;
 	}
 
@@ -5788,7 +5814,7 @@ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int
 
 		for_each_cpu(cpu, cpu_smt_mask(core)) {
 			__cpumask_clear_cpu(cpu, cpus);
-			if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+			if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 				idle = false;
 		}
 
@@ -5817,7 +5843,7 @@ static int select_idle_smt(struct task_struct *p, int target)
 	for_each_cpu(cpu, cpu_smt_mask(target)) {
 		if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 			continue;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			return cpu;
 	}
 
@@ -5882,7 +5908,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 	for_each_cpu_wrap(cpu, cpus, target) {
 		if (!--nr)
 			return -1;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			break;
 	}
 
@@ -5902,14 +5928,14 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	struct sched_domain *sd;
 	int i, recent_used_cpu;
 
-	if (available_idle_cpu(target) || sched_idle_cpu(target))
+	if (is_idle_cpu(target) >= cpu_non_preempted_idle)
 		return target;
 
 	/*
 	 * If the previous CPU is cache affine and idle, don't be stupid:
 	 */
 	if (prev != target && cpus_share_cache(prev, target) &&
-	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
+	    (is_idle_cpu(prev) >= cpu_non_preempted_idle))
 		return prev;
 
 	/* Check a recently used CPU as a potential idle candidate: */
@@ -5917,7 +5943,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	if (recent_used_cpu != prev &&
 	    recent_used_cpu != target &&
 	    cpus_share_cache(recent_used_cpu, target) &&
-	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
+	    is_idle_task(recent_used_cpu) >= cpu_non_preempted_idle &&
 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr)) {
 		/*
 		 * Replace recent_used_cpu with prev as it is a potential
