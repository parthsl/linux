Bottom: 8e92928fe5613c705f3b9facd7e6b85f724145e9
Top:    ab049e02d4691d912e69b8417f5caee161347f40
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-30 14:19:43 +0530

sched/fair: Paraphrase cpu_idle_type


---

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6690845f4d79..c880a8225b56 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5453,6 +5453,32 @@ static unsigned long capacity_of(int cpu)
 	return cpu_rq(cpu)->cpu_capacity;
 }
 
+enum cpu_idle_type {
+	cpu_busy = 0,
+	cpu_preempted_idle
+	cpu_non_preempted_idle,
+	cpu_sched_idle,
+}
+
+/*
+ * is_idle_cpu - is a given CPU idle for enqueuing work.
+ * @cpu: the CPU in question.
+ */
+static int is_idle_cpu(int cpu)
+{
+	if (sched_idle_cpu(cpu))
+		return cpu_sched_idle;
+
+	if (!idle_cpu(cpu))
+		return cpu_busy;
+
+	if (vcpu_is_preempted(cpu))
+		return cpu_non_preempted_idle;
+
+	return cpu_preempted_idle;
+}
+#define available_idle_cpu(cpu) ((is_idle_cpu(cpu)) == cpu_non_preempted_idle)
+
 static void record_wakee(struct task_struct *p)
 {
 	/*
@@ -5756,7 +5782,7 @@ void __update_idle_core(struct rq *rq)
 		if (cpu == core)
 			continue;
 
-		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 			goto unlock;
 	}
 
@@ -5788,7 +5814,7 @@ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int
 
 		for_each_cpu(cpu, cpu_smt_mask(core)) {
 			__cpumask_clear_cpu(cpu, cpus);
-			if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+			if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 				idle = false;
 		}
 
@@ -5817,7 +5843,7 @@ static int select_idle_smt(struct task_struct *p, int target)
 	for_each_cpu(cpu, cpu_smt_mask(target)) {
 		if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 			continue;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			return cpu;
 	}
 
@@ -5882,7 +5908,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 	for_each_cpu_wrap(cpu, cpus, target) {
 		if (!--nr)
 			return -1;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			break;
 	}
 
@@ -5902,14 +5928,14 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	struct sched_domain *sd;
 	int i, recent_used_cpu;
 
-	if (available_idle_cpu(target) || sched_idle_cpu(target))
+	if (is_idle_cpu(target) >= cpu_non_preempted_idle)
 		return target;
 
 	/*
 	 * If the previous CPU is cache affine and idle, don't be stupid:
 	 */
 	if (prev != target && cpus_share_cache(prev, target) &&
-	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
+	    (is_idle_cpu(prev) >= cpu_non_preempted_idle))
 		return prev;
 
 	/* Check a recently used CPU as a potential idle candidate: */
@@ -5917,7 +5943,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	if (recent_used_cpu != prev &&
 	    recent_used_cpu != target &&
 	    cpus_share_cache(recent_used_cpu, target) &&
-	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
+	    is_idle_task(recent_used_cpu) >= cpu_non_preempted_idle &&
 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr)) {
 		/*
 		 * Replace recent_used_cpu with prev as it is a potential
