Bottom: 66e1e85ca5c8da024f82b982fa547f51ce1bdb58
Top:    2ce6dfed659af1acb5c62ca3d867e9b582090479
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-05-14 11:29:10 +0530

sched: Defer updating nr_lat_sensitive update to activate_task process

set_task_cpu() does not hold new_cpu's rq lock, this may result in race
condition when trying to update per_cpu nr_lat_sensitive counter.

Hence defer this counter update to the time when we do activate_task to the
new CPU where we do hold the new_cpu's rq lock.


---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fe774baf7118..2a12a0e74d92 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1493,6 +1493,8 @@ static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf,
 
 	rq_lock(rq, rf);
 	BUG_ON(task_cpu(p) != new_cpu);
+	if (task_is_lat_sensitive(p))
+		per_cpu(nr_lat_sensitive, new_cpu)++;
 	enqueue_task(rq, p, 0);
 	p->on_rq = TASK_ON_RQ_QUEUED;
 	check_preempt_curr(rq, p, 0);
@@ -1744,10 +1746,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 	trace_sched_migrate_task(p, new_cpu);
 
 	if (task_cpu(p) != new_cpu) {
-		if (task_is_lat_sensitive(p)) {
+		if (task_is_lat_sensitive(p))
 			per_cpu(nr_lat_sensitive, task_cpu(p))--;
-			per_cpu(nr_lat_sensitive, new_cpu)++;
-		}
 
 		if (p->sched_class->migrate_task_rq)
 			p->sched_class->migrate_task_rq(p, new_cpu);
@@ -1774,6 +1774,8 @@ static void __migrate_swap_task(struct task_struct *p, int cpu)
 
 		deactivate_task(src_rq, p, 0);
 		set_task_cpu(p, cpu);
+		if (task_is_lat_sensitive(p))
+			per_cpu(nr_lat_sensitive, cpu)++;
 		activate_task(dst_rq, p, 0);
 		check_preempt_curr(dst_rq, p, 0);
 
@@ -2633,6 +2635,8 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 		wake_flags |= WF_MIGRATED;
 		psi_ttwu_dequeue(p);
 		set_task_cpu(p, cpu);
+		if (task_is_lat_sensitive(p))
+			per_cpu(nr_lat_sensitive, cpu)++;
 	}
 
 #else /* CONFIG_SMP */
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 504d2f51b0d6..7a4073ece1b9 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -593,6 +593,8 @@ static struct rq *dl_task_offline_migration(struct rq *rq, struct task_struct *p
 	raw_spin_unlock(&dl_b->lock);
 
 	set_task_cpu(p, later_rq->cpu);
+	if (task_is_lat_sensitive(p))
+		per_cpu(nr_lat_sensitive, rq->cpu)++;
 	double_unlock_balance(later_rq, rq);
 
 	return later_rq;
@@ -2105,6 +2107,10 @@ static int push_dl_task(struct rq *rq)
 	deactivate_task(rq, next_task, 0);
 	set_task_cpu(next_task, later_rq->cpu);
 
+	if (task_is_lat_sensitive(next_task))
+		per_cpu(nr_lat_sensitive, later_rq->cpu)++;
+
+
 	/*
 	 * Update the later_rq clock here, because the clock is used
 	 * by the cpufreq_update_util() inside __add_running_bw().
@@ -2198,6 +2204,9 @@ static void pull_dl_task(struct rq *this_rq)
 
 			deactivate_task(src_rq, p, 0);
 			set_task_cpu(p, this_cpu);
+			if (task_is_lat_sensitive(p))
+				per_cpu(nr_lat_sensitive, this_cpu)++;
+
 			activate_task(this_rq, p, 0);
 			dmin = p->dl.deadline;
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index d7fb20adabeb..8b1be5dadb43 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -7719,6 +7719,10 @@ static void attach_one_task(struct rq *rq, struct task_struct *p)
 	struct rq_flags rf;
 
 	rq_lock(rq, &rf);
+
+	if (task_is_lat_sensitive(p))
+		per_cpu(nr_lat_sensitive, rq->cpu)++;
+
 	update_rq_clock(rq);
 	attach_task(rq, p);
 	rq_unlock(rq, &rf);
@@ -7741,6 +7745,8 @@ static void attach_tasks(struct lb_env *env)
 		p = list_first_entry(tasks, struct task_struct, se.group_node);
 		list_del_init(&p->se.group_node);
 
+		if (task_is_lat_sensitive(p))
+			per_cpu(nr_lat_sensitive, env->dst_cpu)++;
 		attach_task(env->dst_rq, p);
 	}
 
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index df11d88c9895..c826851a5625 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -1924,6 +1924,10 @@ static int push_rt_task(struct rq *rq)
 
 	deactivate_task(rq, next_task, 0);
 	set_task_cpu(next_task, lowest_rq->cpu);
+
+	if (task_is_lat_sensitive(next_task))
+		per_cpu(nr_lat_sensitive, lowest_rq->cpu)++;
+
 	activate_task(lowest_rq, next_task, 0);
 	ret = 1;
 
@@ -2196,6 +2200,8 @@ static void pull_rt_task(struct rq *this_rq)
 
 			deactivate_task(src_rq, p, 0);
 			set_task_cpu(p, this_cpu);
+			if (task_is_lat_sensitive(p))
+				per_cpu(nr_lat_sensitive, this_rq->cpu)++;
 			activate_task(this_rq, p, 0);
 			/*
 			 * We continue with the search, just in
