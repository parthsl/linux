Bottom: 176ad0a74963309e2c364e070b2b5b7c8c0ca430
Top:    66e1e85ca5c8da024f82b982fa547f51ce1bdb58
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-05-14 10:24:03 +0530

Refresh of sched-core-set

---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fde98ebd318b..fe774baf7118 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -3262,9 +3262,6 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
 
-		if (task_is_lat_sensitive(prev))
-			per_cpu(nr_lat_sensitive, prev->cpu)--;
-
 		/*
 		 * Remove function-return probe instances associated with this
 		 * task and put them back on the free list.
@@ -3398,6 +3395,9 @@ context_switch(struct rq *rq, struct task_struct *prev,
 
 	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
 
+	if (prev->state == TASK_DEAD && task_is_lat_sensitive(prev))
+		per_cpu(nr_lat_sensitive, prev->cpu)--;
+
 	prepare_lock_switch(rq, next, rf);
 
 	/* Here we just switch the register state and the stack. */
