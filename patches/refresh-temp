Bottom: 95ad21949183eeb7d4e344f09431274d7a8174a2
Top:    8a5029be9f2f4e64260ab66add42399f55ad8293
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-16 18:20:41 +0530

Refresh of sched-fair-tune-task-wake-up

---

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 6ce6d9803e52..22a45c60d9ab 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5890,7 +5890,8 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 /* Define non-idle CPU as the one with the utilization >= 12.5% */
 #define is_cpu_non_idle(util) ((util) > (100 >> 3))
 
-static inline bool is_background_task(struct task_struct *p)
+/* Classify background tasks with higher latency_nice value for task packing */
+static inline bool is_bg_task(struct task_struct *p)
 {
 	/* Pack a task with utilization >= 12.5% only */
 	if (task_latency_lenient(p) && (task_util(p) > (1024 >> 3)))
@@ -6442,7 +6443,7 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_f
 		}
 
 #ifdef CONFIG_SCHED_SMT
-		if (is_turbosched_enabled() && unlikely(is_background_task(p))) {
+		if (is_turbosched_enabled() && unlikely(is_bg_task(p))) {
 			new_cpu = select_non_idle_core(p, prev_cpu);
 			if (new_cpu >= 0)
 				return new_cpu;
