Bottom: ccf1c3ae432b3ce1789185552df6b20b965cdd01
Top:    b2578af105ba6c38c45331d6fc64dcee25c0fdea
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-31 12:27:36 +0530

Refresh of sched-fair-paraphrase

---

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 36599b6155ea..c880a8225b56 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5453,22 +5453,31 @@ static unsigned long capacity_of(int cpu)
 	return cpu_rq(cpu)->cpu_capacity;
 }
 
+enum cpu_idle_type {
+	cpu_busy = 0,
+	cpu_preempted_idle
+	cpu_non_preempted_idle,
+	cpu_sched_idle,
+}
+
 /*
- * available_idle_cpu - is a given CPU idle for enqueuing work.
+ * is_idle_cpu - is a given CPU idle for enqueuing work.
  * @cpu: the CPU in question.
- *
- * Return: 1 if the CPU is currently idle. 0 otherwise.
  */
-int available_idle_cpu(int cpu)
+static int is_idle_cpu(int cpu)
 {
-       if (!idle_cpu(cpu))
-               return 0;
+	if (sched_idle_cpu(cpu))
+		return cpu_sched_idle;
+
+	if (!idle_cpu(cpu))
+		return cpu_busy;
 
-       if (vcpu_is_preempted(cpu))
-               return 0;
+	if (vcpu_is_preempted(cpu))
+		return cpu_non_preempted_idle;
 
-       return 1;
+	return cpu_preempted_idle;
 }
+#define available_idle_cpu(cpu) ((is_idle_cpu(cpu)) == cpu_non_preempted_idle)
 
 static void record_wakee(struct task_struct *p)
 {
@@ -5773,7 +5782,7 @@ void __update_idle_core(struct rq *rq)
 		if (cpu == core)
 			continue;
 
-		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 			goto unlock;
 	}
 
@@ -5805,7 +5814,7 @@ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int
 
 		for_each_cpu(cpu, cpu_smt_mask(core)) {
 			__cpumask_clear_cpu(cpu, cpus);
-			if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
+			if (is_idle_cpu(cpu) <= cpu_preempted_idle)
 				idle = false;
 		}
 
@@ -5834,7 +5843,7 @@ static int select_idle_smt(struct task_struct *p, int target)
 	for_each_cpu(cpu, cpu_smt_mask(target)) {
 		if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 			continue;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			return cpu;
 	}
 
@@ -5899,7 +5908,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 	for_each_cpu_wrap(cpu, cpus, target) {
 		if (!--nr)
 			return -1;
-		if (available_idle_cpu(cpu) || sched_idle_cpu(cpu))
+		if (is_idle_cpu(cpu) >= cpu_non_preempted_idle)
 			break;
 	}
 
@@ -5919,14 +5928,14 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	struct sched_domain *sd;
 	int i, recent_used_cpu;
 
-	if (available_idle_cpu(target) || sched_idle_cpu(target))
+	if (is_idle_cpu(target) >= cpu_non_preempted_idle)
 		return target;
 
 	/*
 	 * If the previous CPU is cache affine and idle, don't be stupid:
 	 */
 	if (prev != target && cpus_share_cache(prev, target) &&
-	    (available_idle_cpu(prev) || sched_idle_cpu(prev)))
+	    (is_idle_cpu(prev) >= cpu_non_preempted_idle))
 		return prev;
 
 	/* Check a recently used CPU as a potential idle candidate: */
@@ -5934,7 +5943,7 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	if (recent_used_cpu != prev &&
 	    recent_used_cpu != target &&
 	    cpus_share_cache(recent_used_cpu, target) &&
-	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
+	    is_idle_task(recent_used_cpu) >= cpu_non_preempted_idle &&
 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr)) {
 		/*
 		 * Replace recent_used_cpu with prev as it is a potential
