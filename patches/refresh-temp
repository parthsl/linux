Bottom: 63f95128dd0a64087c14695edb00fb4aa6f85c88
Top:    ede82ff246609080b729cfd9089d35e953380878
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-05-07 16:42:41 +0530

Refresh of debug-bits-v2

---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b1df56dd0b05..75b741fde925 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1749,6 +1749,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 			per_cpu(nr_lat_sensitive, task_cpu(p))--;
 			per_cpu(nr_lat_sensitive, new_cpu)++;
 			task_unlock(p);
+			if (sched_feat(MY_DEBUG))
+				trace_printk("this!=new: pid=%d this=%d new=%d dic=%d dic=%d\n", p->pid, task_cpu(p), new_cpu, per_cpu(nr_lat_sensitive, task_cpu(p)), per_cpu(nr_lat_sensitive, new_cpu));
 		}
 
 		if (p->sched_class->migrate_task_rq)
@@ -2978,6 +2980,8 @@ void wake_up_new_task(struct task_struct *p)
 	if (task_is_lat_sensitive(p)) {
 		task_lock(p);
 		per_cpu(nr_lat_sensitive, target_cpu)++;
+		if (sched_feat(MY_DEBUG))
+			trace_printk("wakeup new: pid=%d target_cpu=%d dic=%d\n", p->pid, target_cpu, per_cpu(nr_lat_sensitive, target_cpu));
 		task_unlock(p);
 	}
 #endif
@@ -3270,6 +3274,8 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 		if (task_is_lat_sensitive(prev)) {
 			task_lock(prev);
 			per_cpu(nr_lat_sensitive, prev->cpu)--;
+			if (sched_feat(MY_DEBUG))
+				trace_printk("dead task: pid=%d prevcpu=%d dic=%d\n", prev->pid, prev->cpu, per_cpu(nr_lat_sensitive, prev->cpu));
 			task_unlock(prev);
 		}
 
@@ -4761,10 +4767,16 @@ static void __setscheduler_params(struct task_struct *p,
 		task_lock(p);
 		if (p->state != TASK_DEAD &&
 		    attr->sched_latency_nice != p->latency_nice) {
-			if (attr->sched_latency_nice == MIN_LATENCY_NICE)
+			if (attr->sched_latency_nice == MIN_LATENCY_NICE) {
 				per_cpu(nr_lat_sensitive, task_cpu(p))++;
-			else if (task_is_lat_sensitive(p))
+				if (sched_feat(MY_DEBUG))
+					trace_printk("set lnice: pid=%d vthiscpu=%d dic=%d\n", p->pid, task_cpu(p), per_cpu(nr_lat_sensitive, task_cpu(p)));
+			}
+			else if (task_is_lat_sensitive(p)) {
 				per_cpu(nr_lat_sensitive, task_cpu(p))--;
+				if (sched_feat(MY_DEBUG))
+					trace_printk("unset lnice:pid=%d  thiscpu=%d dic=%d\n", p->pid, task_cpu(p), per_cpu(nr_lat_sensitive, task_cpu(p)));
+			}
 		}
 
 		p->latency_nice = attr->sched_latency_nice;
@@ -6775,6 +6787,8 @@ void __init sched_init(void)
 		hrtick_rq_init(rq);
 		atomic_set(&rq->nr_iowait, 0);
 		per_cpu(nr_lat_sensitive, i) = 0;
+		if (sched_feat(MY_DEBUG))
+			trace_printk("sched init: thiscpu=%d dic=%d\n", i, per_cpu(nr_lat_sensitive, i));
 	}
 
 	set_load_weight(&init_task, false);
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 7481cd96f391..7534539a58b9 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -90,3 +90,5 @@ SCHED_FEAT(WA_BIAS, true)
  */
 SCHED_FEAT(UTIL_EST, true)
 SCHED_FEAT(UTIL_EST_FASTUP, true)
+SCHED_FEAT(MY_DEBUG, true)
+SCHED_FEAT(MY_RESET, false)
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 7aa0775e69c0..4e2baec36cc7 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -236,6 +236,12 @@ static void do_idle(void)
 	if (pm_disabled < 0)
 		pr_info("Inconsistent value of nr_lat_sensitive counter\n");
 
+	if (sched_feat(MY_DEBUG)) {
+		if (pm_disabled != 0)
+			trace_printk("cpu-%d pm_disabled=%d\n",cpu,pm_disabled);
+	}
+	if (sched_feat(MY_RESET))
+		atomic_set(&per_cpu(nr_lat_sensitive, cpu), 0);
 	/*
 	 * If the arch has a polling bit, we maintain an invariant:
 	 *
