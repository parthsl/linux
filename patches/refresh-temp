Bottom: 7a05e2e68c5c3722db8cbc949bbce313a92ebf2d
Top:    1d854efb4a66bf8cc596784039776771644f0039
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-09 14:04:09 +0530

Refresh of sched-fair-provide-arch-hook

---

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 0dc89f149608..a32a3d9b5ec5 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5890,6 +5890,13 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int t
 /* Define non-idle CPU/task as the one with the utilization >= 12.5% */
 #define is_zealous(util) ((util) > (100 >> 3))
 
+#ifndef arch_turbo_domain
+static __always_inline struct cpumask *arch_turbo_domain(int cpu)
+{
+	return sched_domain_span(rcu_dereference(per_cpu(sd_llc, cpu)));
+}
+#endif
+
 static inline bool is_background_task(struct task_struct *p)
 {
 	if (p->latency_tolerance > 18)
@@ -5919,7 +5926,8 @@ static int select_non_idle_core(struct task_struct *p, int prev_cpu, int target)
 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(turbo_sched_mask);
 	int iter_cpu, sibling;
 
-	cpumask_and(cpus, cpu_online_mask, p->cpus_ptr);
+	cpumask_and(cpus, cpu_online_mask, arch_turbo_domain(prev_cpu));
+	cpumask_and(cpus, cpus, p->cpus_ptr);
 
 	for_each_cpu_wrap(iter_cpu, cpus, prev_cpu) {
 		int idle_cpu_count = 0, non_idle_cpu_count = 0;
