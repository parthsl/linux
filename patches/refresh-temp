Bottom: 682bfba7aacdc3bff57b2988fb82f06afbf26bb6
Top:    33693ee5205ca42702481cb1025e82311a24aa36
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-05-07 16:38:41 +0530

Refresh of sched-core-set

---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 9be4e83f7a2b..b1df56dd0b05 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1746,8 +1746,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 	if (task_cpu(p) != new_cpu) {
 		if (task_is_lat_sensitive(p)) {
 			task_lock(p);
-			atomic_dec(&per_cpu(nr_lat_sensitive, task_cpu(p)));
-			atomic_inc(&per_cpu(nr_lat_sensitive, new_cpu));
+			per_cpu(nr_lat_sensitive, task_cpu(p))--;
+			per_cpu(nr_lat_sensitive, new_cpu)++;
 			task_unlock(p);
 		}
 
@@ -2977,7 +2977,7 @@ void wake_up_new_task(struct task_struct *p)
 #ifdef CONFIG_SMP
 	if (task_is_lat_sensitive(p)) {
 		task_lock(p);
-		atomic_inc(&per_cpu(nr_lat_sensitive, target_cpu));
+		per_cpu(nr_lat_sensitive, target_cpu)++;
 		task_unlock(p);
 	}
 #endif
@@ -3269,7 +3269,7 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 
 		if (task_is_lat_sensitive(prev)) {
 			task_lock(prev);
-			atomic_dec(&per_cpu(nr_lat_sensitive, prev->cpu));
+			per_cpu(nr_lat_sensitive, prev->cpu)--;
 			task_unlock(prev);
 		}
 
@@ -4762,9 +4762,9 @@ static void __setscheduler_params(struct task_struct *p,
 		if (p->state != TASK_DEAD &&
 		    attr->sched_latency_nice != p->latency_nice) {
 			if (attr->sched_latency_nice == MIN_LATENCY_NICE)
-				atomic_inc(&per_cpu(nr_lat_sensitive, task_cpu(p)));
+				per_cpu(nr_lat_sensitive, task_cpu(p))++;
 			else if (task_is_lat_sensitive(p))
-				atomic_dec(&per_cpu(nr_lat_sensitive, task_cpu(p)));
+				per_cpu(nr_lat_sensitive, task_cpu(p))--;
 		}
 
 		p->latency_nice = attr->sched_latency_nice;
