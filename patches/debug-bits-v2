Bottom: 8012efa88ee62f204031db0385dbb981d78427a7
Top:    9dee1f570e601640bfb3075d0be0cbd4c640d084
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-04-16 12:36:44 +0530

debug bits v2

---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index fde98ebd318b..52b91af77019 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1747,6 +1747,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 		if (task_is_lat_sensitive(p)) {
 			per_cpu(nr_lat_sensitive, task_cpu(p))--;
 			per_cpu(nr_lat_sensitive, new_cpu)++;
+			if (sched_feat(MY_DEBUG))
+				trace_printk("this!=new: pid=%d this=%d new=%d dic=%d dic=%d\n", p->pid, task_cpu(p), new_cpu, per_cpu(nr_lat_sensitive, task_cpu(p)), per_cpu(nr_lat_sensitive, new_cpu));
 		}
 
 		if (p->sched_class->migrate_task_rq)
@@ -2973,8 +2975,11 @@ void wake_up_new_task(struct task_struct *p)
 	rq = __task_rq_lock(p, &rf);
 
 #ifdef CONFIG_SMP
-	if (task_is_lat_sensitive(p))
+	if (task_is_lat_sensitive(p)) {
 		per_cpu(nr_lat_sensitive, target_cpu)++;
+		if (sched_feat(MY_DEBUG))
+			trace_printk("wakeup new: pid=%d target_cpu=%d dic=%d\n", p->pid, target_cpu, per_cpu(nr_lat_sensitive, target_cpu));
+	}
 #endif
 
 	update_rq_clock(rq);
@@ -3262,8 +3267,11 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
 
-		if (task_is_lat_sensitive(prev))
+		if (task_is_lat_sensitive(prev)) {
 			per_cpu(nr_lat_sensitive, prev->cpu)--;
+			if (sched_feat(MY_DEBUG))
+				trace_printk("dead task: pid=%d prevcpu=%d dic=%d\n", prev->pid, prev->cpu, per_cpu(nr_lat_sensitive, prev->cpu));
+		}
 
 		/*
 		 * Remove function-return probe instances associated with this
@@ -4767,10 +4775,16 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	if (attr->sched_flags & SCHED_FLAG_LATENCY_NICE) {
 		if (p->state != TASK_DEAD &&
 		    attr->sched_latency_nice != p->latency_nice) {
-			if (attr->sched_latency_nice == MIN_LATENCY_NICE)
+			if (attr->sched_latency_nice == MIN_LATENCY_NICE) {
 				per_cpu(nr_lat_sensitive, task_cpu(p))++;
-			else if (task_is_lat_sensitive(p))
+				if (sched_feat(MY_DEBUG))
+					trace_printk("set lnice: pid=%d vthiscpu=%d dic=%d\n", p->pid, task_cpu(p), per_cpu(nr_lat_sensitive, task_cpu(p)));
+			}
+			else if (task_is_lat_sensitive(p)) {
 				per_cpu(nr_lat_sensitive, task_cpu(p))--;
+				if (sched_feat(MY_DEBUG))
+					trace_printk("unset lnice:pid=%d  thiscpu=%d dic=%d\n", p->pid, task_cpu(p), per_cpu(nr_lat_sensitive, task_cpu(p)));
+			}
 		}
 
 		p->latency_nice = attr->sched_latency_nice;
@@ -6767,6 +6781,8 @@ void __init sched_init(void)
 		hrtick_rq_init(rq);
 		atomic_set(&rq->nr_iowait, 0);
 		per_cpu(nr_lat_sensitive, i) = 0;
+		if (sched_feat(MY_DEBUG))
+			trace_printk("sched init: thiscpu=%d dic=%d\n", i, per_cpu(nr_lat_sensitive, i));
 	}
 
 	set_load_weight(&init_task, false);
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 7481cd96f391..7534539a58b9 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -90,3 +90,5 @@ SCHED_FEAT(WA_BIAS, true)
  */
 SCHED_FEAT(UTIL_EST, true)
 SCHED_FEAT(UTIL_EST_FASTUP, true)
+SCHED_FEAT(MY_DEBUG, true)
+SCHED_FEAT(MY_RESET, false)
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 7aa0775e69c0..fc9d19afdd88 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -236,6 +236,12 @@ static void do_idle(void)
 	if (pm_disabled < 0)
 		pr_info("Inconsistent value of nr_lat_sensitive counter\n");
 
+	if (sched_feat(MY_DEBUG)) {
+		if (pm_disabled != 0)
+			trace_printk("cpu-%d pm_disabled=%d\n",cpu,pm_disabled);
+	}
+	if (sched_feat(MY_RESET))
+		per_cpu(nr_lat_sensitive, cpu) = 0;
 	/*
 	 * If the arch has a polling bit, we maintain an invariant:
 	 *
