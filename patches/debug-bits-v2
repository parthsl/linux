Bottom: b967c99a1264a3953e5343a78eb32be3ab16c0b9
Top:    91f989c7c3afa15327ee813761f6b235e5c7346e
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-04-16 12:36:44 +0530

debug bits v2

---

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f74c307e56c1..b2d3b4f6097b 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1749,6 +1749,8 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 			atomic_dec(&per_cpu(nr_lat_sensitive, task_cpu(p)));
 			atomic_inc(&per_cpu(nr_lat_sensitive, new_cpu));
 			task_unlock(p);
+			if (sched_feat(MY_DEBUG))
+				trace_printk("this!=new: pid=%d this=%d new=%d dic=%d dic=%d\n", p->pid, task_cpu(p), new_cpu, atomic_read(&per_cpu(nr_lat_sensitive, task_cpu(p))), atomic_read(&per_cpu(nr_lat_sensitive, new_cpu)));
 		}
 
 		if (p->sched_class->migrate_task_rq)
@@ -2978,6 +2980,8 @@ void wake_up_new_task(struct task_struct *p)
 	if (task_is_lat_sensitive(p)) {
 		task_lock(p);
 		atomic_inc(&per_cpu(nr_lat_sensitive, target_cpu));
+		if (sched_feat(MY_DEBUG))
+			trace_printk("wakeup new: pid=%d target_cpu=%d dic=%d\n", p->pid, target_cpu, atomic_read(&per_cpu(nr_lat_sensitive, target_cpu)));
 		task_unlock(p);
 	}
 #endif
@@ -3270,6 +3274,8 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 		if (task_is_lat_sensitive(prev)) {
 			task_lock(prev);
 			atomic_dec(&per_cpu(nr_lat_sensitive, prev->cpu));
+			if (sched_feat(MY_DEBUG))
+				trace_printk("dead task: pid=%d prevcpu=%d dic=%d\n", prev->pid, prev->cpu, atomic_read(&per_cpu(nr_lat_sensitive, prev->cpu)));
 			task_unlock(prev);
 		}
 
@@ -4761,10 +4767,16 @@ static void __setscheduler_params(struct task_struct *p,
 		task_lock(p);
 		if (p->state != TASK_DEAD &&
 		    attr->sched_latency_nice != p->latency_nice) {
-			if (attr->sched_latency_nice == MIN_LATENCY_NICE)
+			if (attr->sched_latency_nice == MIN_LATENCY_NICE) {
 				atomic_inc(&per_cpu(nr_lat_sensitive, task_cpu(p)));
-			else if (task_is_lat_sensitive(p))
+				if (sched_feat(MY_DEBUG))
+					trace_printk("set lnice: pid=%d vthiscpu=%d dic=%d\n", p->pid, task_cpu(p), atomic_read(&per_cpu(nr_lat_sensitive, task_cpu(p))));
+			}
+			else if (task_is_lat_sensitive(p)) {
 				atomic_dec(&per_cpu(nr_lat_sensitive, task_cpu(p)));
+				if (sched_feat(MY_DEBUG))
+					trace_printk("unset lnice:pid=%d  thiscpu=%d dic=%d\n", p->pid, task_cpu(p), atomic_read(&per_cpu(nr_lat_sensitive, task_cpu(p))));
+			}
 		}
 
 		p->latency_nice = attr->sched_latency_nice;
@@ -6775,6 +6787,8 @@ void __init sched_init(void)
 		hrtick_rq_init(rq);
 		atomic_set(&rq->nr_iowait, 0);
 		atomic_set(&per_cpu(nr_lat_sensitive, i), 0);
+		if (sched_feat(MY_DEBUG))
+			trace_printk("sched init: thiscpu=%d dic=%d\n", i, atomic_read(&per_cpu(nr_lat_sensitive, i)));
 	}
 
 	set_load_weight(&init_task, false);
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index 7481cd96f391..7534539a58b9 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -90,3 +90,5 @@ SCHED_FEAT(WA_BIAS, true)
  */
 SCHED_FEAT(UTIL_EST, true)
 SCHED_FEAT(UTIL_EST_FASTUP, true)
+SCHED_FEAT(MY_DEBUG, true)
+SCHED_FEAT(MY_RESET, false)
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 6581613189d9..658d43a549b1 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -236,6 +236,12 @@ static void do_idle(void)
 	if (pm_disabled < 0)
 		pr_info("Inconsistent value of nr_lat_sensitive counter\n");
 
+	if (sched_feat(MY_DEBUG)) {
+		if (pm_disabled != 0)
+			trace_printk("cpu-%d pm_disabled=%d\n",cpu,pm_disabled);
+	}
+	if (sched_feat(MY_RESET))
+		atomic_set(&per_cpu(nr_lat_sensitive, cpu), 0);
 	/*
 	 * If the arch has a polling bit, we maintain an invariant:
 	 *
