Bottom: d1b31b36ae61f7143fb85c0a377bd33d474eb2ef
Top:    8e92928fe5613c705f3b9facd7e6b85f724145e9
Author: Parth Shah <parth@linux.ibm.com>
Date:   2020-01-30 10:56:10 +0530

sched/fair: Consider SCHED_IDLE CPUs for select_idle_core

select_idle_core() searches for a core in sd_llc with all the
available_idle_cpu(), which don't count the sched_idle_cpu(). Hence
consider the CPUs running only SCHED_IDLE class tasks for task wakeup.

The __update_idle_core() keeps track of the presence of any idle
core in the sd_llc. So align this with sched_idle_cpu selection.

Signed-off-by: Parth Shah <parth@linux.ibm.com>


---

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index fe4e0d775375..6690845f4d79 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5756,7 +5756,7 @@ void __update_idle_core(struct rq *rq)
 		if (cpu == core)
 			continue;
 
-		if (!available_idle_cpu(cpu))
+		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
 			goto unlock;
 	}
 
@@ -5788,7 +5788,7 @@ static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int
 
 		for_each_cpu(cpu, cpu_smt_mask(core)) {
 			__cpumask_clear_cpu(cpu, cpus);
-			if (!available_idle_cpu(cpu))
+			if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
 				idle = false;
 		}
